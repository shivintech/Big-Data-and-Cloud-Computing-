# -*- coding: utf-8 -*-
"""Final_sga113 Data 301 assignemnt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ALyHVDkHN_mW97YGvRBNh0M9CB5ur7KE

## setup libraries
"""

#library and code setup
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip install -q pyspark
!pip install newspaper3k

import pyspark, os
from pyspark import SparkConf, SparkContext
os.environ["PYSPARK_PYTHON"]="python3"
os.environ["JAVA_HOME"]="/usr/lib/jvm/java-8-openjdk-amd64/"

#start spark local server
import sys, os
from operator import add
import time

os.environ["PYSPARK_PYTHON"]="python3"

import pyspark
from pyspark import SparkConf, SparkContext

#connects our python driver to a local Spark JVM running on the Google Colab server virtual machine
try:
  conf = SparkConf().setMaster("local[*]").set("spark.executor.memory", "1g")
  sc = SparkContext(conf = conf)
except ValueError:
  #it's ok if the server is already started
  pass

def dbg(x):
  """ A helper function to print debugging information on RDDs """
  if isinstance(x, pyspark.RDD):
    print([(t[0], list(t[1]) if 
            isinstance(t[1], pyspark.resultiterable.ResultIterable) else t[1])
           if isinstance(t, tuple) else t
           for t in x.take(100)])
  else:
    print(x)

queryURLbase = "https://api.gdeltproject.org/api/v2/doc/doc?format=csv&query=(unemployment%20or%20coronavirus%20or%20banksor%20loansor%20jobs)%20%20%20sourcecountry:US%20sourcelang:eng&mode=artlist&maxrecords=250&sort=hybridrel"

# these remove files from previous runs
!rm -rf articles
!rm *.csv
!rm -rf 2020files.csv
!rm -rf 2018files.csv
!rm -rf attempt12020files.csv
!rm -rf attempt22020files.csv
!rm -rf attempt1
!rm -rf attempt2
!rm -rf 2020.txt

from datetime import date, timedelta
import pandas as pd
import os
import urllib.request
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
import numpy as np
from newspaper import Article
import nltk
nltk.download('popular', quiet=True)


# generic functions to pull and write data to disk based on date
def get_filename(x):
  date = x.strftime('%Y%m%d')
  return "{}_gdeltdata.csv".format(date)

def intofile(filename, name):
    try:
        if not os.path.exists(filename):
          date = filename.split("_")[0]
          startdate = date+"000000" #0 hour 0 min 0 sec (start of day)
          enddate = date+"235959" #23 hour 59 min 59 sec (end of day)
          query = queryURLbase + "&startdatetime=" + startdate + "&enddatetime=" + enddate
          with urllib.request.urlopen(query) as testfile, open("/content/" + name + "/" + filename, 'w') as f:
            f.write(testfile.read().decode())
    except:
        print("Error occurred")

"""## read downloaded files into RDDs

##Use the newspaper3k library from python to scrape the keywords
"""

import numpy as np
from newspaper import Article
import nltk
import pandas as pd
nltk.download('popular', quiet=True)


def get_urls(row):
    return row['URL']
       
def write_wget(withid):
  url = withid[0]
  id = withid[1]
  s = 'articles/' + str(id) + ".html " + url
  if not os.path.exists(os.getcwd()+'/'+s):
    print("wget " + s)
    os.system('wget -O ' + s)
  return id


def output_keywords(id):
  try:
    filename = "articles/"+str(id)+".html"
    filenameurl = "file://"+os.getcwd()+'/'+filename
    article = Article(filenameurl)
    with open(filename, 'r') as f:
      s = f.read()
      article.set_html(s)
    article.parse()
    article.nlp()
    return [k for k in article.keywords]
  except Exception as e:
    print("Exception: " + str(e))
    return []

# The combine_files() function below the connects all the .csv files together and generates a single file containing the URLS
# for all the different dates within the specified date range.

def combine_files(year, name):
  """The function combines all the .csv files for specifid date range and writes it to a single csv file for the specified date range."""
  combined_files = sqlContext.read.csv('/content/' + name + '/'  + "*.csv", header=True) #combines all the files the with the csv 
  combined_files.write.csv('/content/' + name + year + "files.csv", header = 'true' )
  print("Total number urls in combined file: " + str(combined_files.count()) + " urls")
  totalfiles = sqlContext.read.csv('/content/' + name + year + "files.csv", header=True) #loads the combined file back in
  return totalfiles

# The generateFiles() uses the newspaper3k library which fetches the keywords from the article  
# Each article is considered here as a basket and set of keywords for each article for the given ste of dates is saved to a .txt file.
# The below process takes around 1.5 hours when ran on a time frame of 25 days, so a total of 3 hours in to retreive the data 
# from January 2020 and April 2020.
def generatingFiles(startDate, endDate, year, name):
   print("Data analysis for the year {}".format(year))
   print()
   daterange = sc.parallelize(pd.date_range(startDate,endDate))
   dates = daterange.map(get_filename)
   for i in dates.collect():
     intofile(i, name)
   new_dates = combine_files(year, name)
   all_urls = new_dates.rdd.map(get_urls) 
   withids = all_urls.zipWithUniqueId()
   print("Total number of articles used to extract the data:", len(all_urls.collect()))
   if not os.path.exists('articles'):
     os.mkdir('articles')
   allids = withids.map(write_wget)
   dbg(allids)
   all_keywords = allids.map(output_keywords)  
   all_keywords.saveAsTextFile(year +".txt")
   dbg(type(all_keywords))
   end_time = time.monotonic()

!rm -rf articles
!rm *.csv
!rm -rf 2020files.csv
!mkdir attempt2
data_april = generatingFiles('2020 Apr 01','2020 Apr 26', '2020' , 'attempt2')

!rm -rf articles
!rm *.csv
!rm -rf 2020files.csv
!mkdir attempt1
data_january = generatingFiles('2020 Jan 01','2020 Jan 26', '2020' , 'attempt1')

def filtering_and_making_pairs(filtered_data):
  """ This function makes the key value pairs here  """

  broadcastData = filtered_data.flatMap(lambda line: line.strip().split(" "))
  broadcastDataWithCounter = broadcastData.map(lambda x: (x,1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x:x[1], False)
  print("Key value pairs as below in descending order:")
  filterBroadcastingData = broadcastDataWithCounter.filter(lambda eachWord : eachWord[1] >= 10)
  print("Top 20 key values pairs and their count:")
  dbg(filterBroadcastingData.take(20))
  return filterBroadcastingData
  
def creating_broadcasting_pairs(data):
  "This function broadcasts the key value pairs"
  data_for_broadcast = filtering_and_making_pairs(data)
  broadcasted_data = sc.broadcast(data_for_broadcast.collectAsMap())
  return broadcasted_data


print("Data for January 2020:")
f1 = sc.textFile("jan01-26.txt")
file_january = (f1.map(lambda r: eval(r)).filter(lambda r : len(r) > 0).map(lambda r: ' '.join(r)))
broadcasted_pairs_january = creating_broadcasting_pairs(file_january)

print("------------------------------------------------------------------------------------------------------------------------------")

print("Data for April 2020:")
f2 = sc.textFile("april01-26")
file_april = (f2.map(lambda r: eval(r)).filter(lambda r : len(r) > 0).map(lambda r: ' '.join(r)))
broadcasted_pairs_april = creating_broadcasting_pairs(file_april)

# This function checks all the rdd pairs against the bradcasted_pairs and only returns the ones which also exists in the broadcasted_pairs
def check_pairs(eachBasket, broadcastedData):

   print("Checking  keywords pairs against broadcasted pairs")
   result = [] 
   for i, m in enumerate(eachBasket):
     for j, n in enumerate(eachBasket): 
       if (m != n) and j < i and m in broadcastedData.value and n in broadcastedData.value: 
         result.append(tuple(sorted((m,n))))
   return result

def check_broadcast_data(input_rdd, broadcastedData):
   Check_against_broadcast = input_rdd.flatMap(lambda eachBasket: check_pairs(([word for word in eachBasket.strip().split(" ")]), broadcastedData))
   count_pair_xy = Check_against_broadcast.map(lambda item: (item,1)).reduceByKey(lambda a, b: a+b).sortBy(lambda x:x[1], False)
   dbg(count_pair_xy)
   return count_pair_xy

print("Keyword pairs for January 2020 after check against broadcasted data")
januaryData = check_broadcast_data(file_january, broadcasted_pairs_january)
print("-------------------------------------------------------------------------")
print()
print("Keyword pairs for April 2020 after check against broadcasted data")
aprilData = check_broadcast_data(file_april, broadcasted_pairs_april)

def confidence(x, broadcastedData):
  print("Calculating confidence")
  return tuple((x[0], x[1] /broadcastedData.value[x[0][1]]))



print("Confidence in January")
count_pair_yx = januaryData.map(lambda x: tuple(((x[0][1], x[0][0]), x[1])))
counting_final_pairs_january = januaryData.union(count_pair_yx).filter(lambda x: x[1] >= 150)
januaryconfidence = counting_final_pairs_january.map(lambda x: tuple(confidence(x, broadcasted_pairs_january))).sortBy(lambda key: key[1], False)
dbg(januaryconfidence)
print("-----------------------------------------------------------------------------------------------------------------------------")
print()
print("Confidence in April")
count_pair_yx = aprilData.map(lambda x: tuple(((x[0][1], x[0][0]), x[1])))
counting_final_pairs_april = aprilData.union(count_pair_yx).filter(lambda x: x[1] >= 150)
aprilConfidence = counting_final_pairs_april.map(lambda x: tuple(confidence(x, broadcasted_pairs_april))).sortBy(lambda key: key[1], False)
dbg(aprilConfidence)

# The function below calculates the interest score by applying the interesting pair rules.

def interest(tups, broadcasted_pairs_data, total_baskets):
  print("Calculating interet score")
  return tups[1][0] - broadcasted_pairs_data.value[tups[0][1]] / total_baskets

def findInterest(file, confidenceData, checkedData, broadcasted_pairs_data):
  num_baskets = file.map(lambda x: x.split(","))
  total_baskets = num_baskets.count()
  join_RDD = confidenceData.join(checkedData)
  interest_score = join_RDD.map(lambda x: (x[0], interest(x, broadcasted_pairs_data, total_baskets))).sortBy(lambda key: key[1], False)
  dbg(interest_score)
  return interest_score


print("Interesting pairs and scores for January 2020")
januaryInterest = findInterest(file_january, januaryconfidence, counting_final_pairs_january, broadcasted_pairs_january)
print("-----------------------------------------------------------------------------------------------------------------------------")
print()
print("Interesting pairs and scores for April 2020")
aprilInterest = findInterest(file_april, aprilConfidence, counting_final_pairs_april, broadcasted_pairs_april)

# The function below makes a list of common interest scores for the pairs for Jan2020 and April2020 
january_interest_values = [] 
april_interest_values = [] 
listJanuary = januaryInterest.collect()
listApril = aprilInterest.collect()
common_pairs = []

for i in range(len(listApril)):
  trigger = True
  for j in range(len(listJanuary)): 
    if ((listApril[i][0] == listJanuary[j][0]) and trigger):
      trigger = False
      april_interest_values.append(listApril[i][1])
      january_interest_values.append(listJanuary[j][1])
      common_pairs.append((listJanuary[j],listApril[i]))
  if(trigger):
    january_interest_values.append(0)
    april_interest_values.append(0)

print("List of january interest values :",  january_interest_values)
print("List of april interest values :", april_interest_values)

# The function below checks the change in the percentage between the interest socre of the common pairs between January 2020 to April 2020
print("Percent change in the interest scores for most interesting pairs in the U.S economy between January 2020 and April 2020 ")
print()
listofpercentchange = []
for a, b in common_pairs:
   x = (b[1] - a[1])/ b[1]* 100 
   listofpercentchange.append((a[0],x))

sortedlist = sorted(listofpercent, key=lambda item: abs(item[1]), reverse = True) 
for i in listofpercentchange:
  print(i[0],": ",  i[1], "%")

# This function calculates how similar was the overall month of January 2020 to April 2020.
import numpy as np
def cosine_similarity(x,y):
  a = np.array(x)
  b = np.array(y)
  numerator = np.dot(a,b)
  mag_x = np.sqrt(a.dot(a))
  mag_y = np.sqrt(b.dot(b))
  denominator = mag_x * mag_y
  return numerator/denominator
print("The cosine similarity in US between the month for January and April:",cosine_similarity(january_interest_values,april_interest_values))